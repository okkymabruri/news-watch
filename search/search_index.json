{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"news-watch: Indonesia's top news websites scraper","text":"<p>news-watch scrapes structured news data from Indonesia's top news websites with keyword and date filtering.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install news-watch\nplaywright install chromium\n</code></pre> <p>Development setup: https://okky.dev/news-watch/getting-started/</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>newswatch --keywords ihsg --start_date 2025-01-01\n</code></pre> <pre><code>import newswatch as nw\n\ndf = nw.scrape_to_dataframe(\"ihsg\", \"2025-01-01\")\nprint(len(df))\n</code></pre>"},{"location":"#docs","title":"Docs","text":"<ul> <li>Getting Started</li> <li>Comprehensive Guide</li> <li>API Reference</li> <li>Troubleshooting</li> <li>Changelog</li> </ul>"},{"location":"#supported-news-sources","title":"Supported News Sources","text":"Source Domain Antara News antaranews.com Bisnis.com bisnis.com Bloomberg Technoz www.bloombergtechnoz.com CNBC Indonesia www.cnbcindonesia.com CNN Indonesia www.cnnindonesia.com Detik detik.com IDN Times idntimes.com Jawa Pos jawapos.com Katadata katadata.co.id Kompas kompas.com Kontan kontan.co.id Kumparan kumparan.com Liputan6 www.liputan6.com Media Indonesia mediaindonesia.com Merdeka merdeka.com Metro TV News metrotvnews.com Okezone okezone.com Republika republika.co.id Suara suara.com Tempo tempo.co Tirto tirto.id Tribunnews www.tribunnews.com Viva viva.co.id"},{"location":"#important-considerations","title":"Important Considerations","text":"<p>Ethical Use: Always respect website terms of service and implement appropriate delays between requests.</p> <p>Performance: Works best in local environments. Cloud platforms may experience reduced performance due to anti-bot measures.</p>"},{"location":"api-reference/","title":"API Reference","text":""},{"location":"api-reference/#quick-reference","title":"Quick Reference","text":"<pre><code>import newswatch as nw\n\n# Core scraping functions\narticles = nw.scrape(\"bank\", \"2025-01-01\")                    # Returns list of dicts\ndf = nw.scrape_to_dataframe(\"bank\", \"2025-01-01\")            # Returns pandas DataFrame  \nnw.scrape_to_file(\"bank\", \"2025-01-01\", \"output.xlsx\")       # Saves to file\n\n# Convenience functions\nscrapers = nw.list_scrapers()                                # Get available scrapers\nrecent_df = nw.quick_scrape(\"politik\", days_back=3)          # Recent articles\n</code></pre>"},{"location":"api-reference/#core-functions","title":"Core Functions","text":""},{"location":"api-reference/#scrape","title":"scrape()","text":"<p>The foundation function that returns raw article data.</p> <pre><code>def scrape(keywords, start_date, scrapers=\"auto\", verbose=False, timeout=300, **kwargs):\n    ...\n</code></pre> <p>Parameters:</p> <ul> <li><code>keywords</code> (str): What to search for. Use commas for multiple terms: <code>\"bank,kredit,fintech\"</code></li> <li><code>start_date</code> (str): When to start looking, in YYYY-MM-DD format: <code>\"2025-01-01\"</code></li> <li><code>scrapers</code> (str, optional): Which sites to scrape:</li> <li><code>\"auto\"</code> (default) - Let news-watch pick based on your platform</li> <li><code>\"all\"</code> - Try every scraper (might fail on some systems)</li> <li><code>\"kompas,detik\"</code> - Pick specific sites by name</li> <li><code>verbose</code> (bool, optional): Show progress details (default: False)</li> <li><code>timeout</code> (int, optional): Max seconds to wait (default: 300)</li> </ul> <p>Returns:</p> <p>List of dictionaries, each containing:</p> <ul> <li><code>title</code> - Article headline</li> <li><code>author</code> - Writer name (when available)</li> <li><code>publish_date</code> - When it was published</li> <li><code>content</code> - Full article text</li> <li><code>keyword</code> - Which search term matched</li> <li><code>category</code> - Article section (news, business, etc.)</li> <li><code>source</code> - Website name</li> <li><code>link</code> - Original URL</li> </ul> <p>Example: <pre><code>import newswatch as nw\n\n# Basic search\narticles = nw.scrape(\"bank\", \"2025-01-01\")\nprint(f\"Found {len(articles)} articles\")\n\n# More specific search\nfinancial_articles = nw.scrape(\n    keywords=\"ihsg,saham,obligasi\",\n    start_date=\"2025-01-15\", \n    scrapers=\"bisnis,kontan\",\n    verbose=True\n)\n\n# Process the raw data\nfor article in financial_articles:\n    print(f\"{article['title']} - {article['source']}\")\n    if \"ihsg\" in article['title'].lower():\n        print(\"  -&gt; Stock market related!\")\n</code></pre></p>"},{"location":"api-reference/#scrape_to_dataframe","title":"scrape_to_dataframe()","text":"<p>Returns a pandas DataFrame ready for analysis.</p> <pre><code>def scrape_to_dataframe(keywords, start_date, scrapers=\"auto\", verbose=False, timeout=300, **kwargs):\n    ...\n</code></pre> <p>Parameters: Same as <code>scrape()</code> function.</p> <p>Returns: pandas DataFrame with the same columns as <code>scrape()</code>, but with <code>publish_date</code> automatically converted to datetime for easy filtering and analysis.</p> <p>Example: <pre><code>import newswatch as nw\nimport pandas as pd\n\n# Get DataFrame for analysis\ndf = nw.scrape_to_dataframe(\"teknologi\", \"2025-01-01\")\n\n# Immediate pandas operations\nprint(f\"Articles per source:\")\nprint(df['source'].value_counts())\n\nprint(f\"Date range: {df['publish_date'].min()} to {df['publish_date'].max()}\")\n\n# Filter and analyze\nrecent = df[df['publish_date'] &gt;= '2025-01-15']\nprint(f\"Recent articles: {len(recent)}\")\n\n# Word count analysis\ndf['word_count'] = df['content'].str.split().str.len()\navg_length = df.groupby('source')['word_count'].mean()\nprint(\"Average article length by source:\")\nprint(avg_length.sort_values(ascending=False))\n</code></pre></p>"},{"location":"api-reference/#scrape_to_file","title":"scrape_to_file()","text":"<p>Save results directly to CSV or Excel files.</p> <pre><code>def scrape_to_file(keywords, start_date, output_path, output_format=\"xlsx\", \n                  scrapers=\"auto\", verbose=False, timeout=300, **kwargs):\n    ...\n</code></pre> <p>Parameters: - <code>keywords</code>, <code>start_date</code>, <code>scrapers</code>, <code>verbose</code>, <code>timeout</code>: Same as other functions - <code>output_path</code> (str): Where to save the file - <code>output_format</code> (str, optional): <code>\"xlsx\"</code>, <code>\"csv\"</code>, or <code>\"json\"</code> (default: \"xlsx\")</p> <p>Returns: Nothing - file is saved to the specified location.</p> <p>Example: <pre><code>import newswatch as nw\n\n# Save as Excel (default)\nnw.scrape_to_file(\n    keywords=\"ekonomi,inflasi\", \n    start_date=\"2025-01-01\",\n    output_path=\"economic_news.xlsx\"\n)\n\n# Save as CSV with specific sources\nnw.scrape_to_file(\n    keywords=\"startup,unicorn,fintech\", \n    start_date=\"2025-01-01\",\n    output_path=\"/path/to/startup_news.csv\",\n    output_format=\"csv\",\n    scrapers=\"detik,kompas\",\n    verbose=True\n)\n\n# Save as JSON for API integration\nnw.scrape_to_file(\n    keywords=\"fintech,digital\", \n    start_date=\"2025-01-01\",\n    output_path=\"tech_articles.json\",\n    output_format=\"json\",\n    scrapers=\"detik,kompas\",\n    verbose=True\n)\n</code></pre></p>"},{"location":"api-reference/#utility-functions","title":"Utility Functions","text":""},{"location":"api-reference/#list_scrapers","title":"list_scrapers()","text":"<p>Find out which Indonesian news sites are available.</p> <pre><code>def list_scrapers():\n    ...\n</code></pre> <p>Returns: List of scraper names you can use with the <code>scrapers</code> parameter.</p> <p>Example: <pre><code>import newswatch as nw\n\navailable = nw.list_scrapers()\nprint(\"Available news sources:\", available)\n# Output: ['antaranews', 'bisnis', 'bloombergtechnoz', 'cnbcindonesia', 'detik', ...]\n\n# Use specific ones for financial news\nfinancial_sources = [\"bisnis\", \"kontan\", \"cnbcindonesia\"]\ndf = nw.scrape_to_dataframe(\"saham\", \"2025-01-01\", scrapers=\",\".join(financial_sources))\n</code></pre></p>"},{"location":"api-reference/#quick_scrape","title":"quick_scrape()","text":"<p>Get recent news without worrying about exact dates.</p> <pre><code>def quick_scrape(keywords, days_back=1, scrapers=\"auto\"):\n    ...\n</code></pre> <p>Parameters: - <code>keywords</code> (str): What to search for - <code>days_back</code> (int, optional): How many days back to look (default: 1) - <code>scrapers</code> (str, optional): Which sources to use (default: \"auto\")</p> <p>Returns: pandas DataFrame with recent articles.</p> <p>Example: <pre><code>import newswatch as nw\n\n# Yesterday's political news\npolitics = nw.quick_scrape(\"politik\")\n\n# Tech news from the last week\ntech_week = nw.quick_scrape(\"teknologi,startup\", days_back=7)\n\n# Banking news from last 3 days, specific sources\nbanking = nw.quick_scrape(\n    \"bank,kredit\", \n    days_back=3, \n    scrapers=\"bisnis,detik\"\n)\n\nprint(f\"Found {len(banking)} banking articles in last 3 days\")\n</code></pre></p>"},{"location":"api-reference/#working-with-multiple-keywords","title":"Working with Multiple Keywords","text":"<p>You can search for multiple topics at once:</p> <pre><code>import newswatch as nw\n\n# Multiple related terms\nbanking = nw.scrape_to_dataframe(\"bank,bca,mandiri,bri,bni\", \"2025-01-01\")\n\n# See which keyword matched each article\nkeyword_counts = banking['keyword'].value_counts()\nprint(\"Articles found per keyword:\")\nprint(keyword_counts)\n\n# Filter by specific keyword\nbca_articles = banking[banking['keyword'] == 'bca']\nprint(f\"BCA-specific articles: {len(bca_articles)}\")\n</code></pre>"},{"location":"api-reference/#error-handling","title":"Error Handling","text":"<p>The API includes structured error handling:</p> <pre><code>import newswatch as nw\nfrom newswatch.exceptions import ValidationError, NewsWatchError\n\ntry:\n    df = nw.scrape_to_dataframe(\"invalid-keyword\", \"not-a-date\")\nexcept ValidationError as e:\n    print(f\"Input validation failed: {e}\")\nexcept NewsWatchError as e:\n    print(f\"Scraping error occurred: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"api-reference/#advanced-usage-examples","title":"Advanced Usage Examples","text":"<p>For comprehensive examples including comparative analysis, time series analysis, content analysis, error handling best practices, integration patterns, and troubleshooting guides, see our Comprehensive Guide.</p> <p>The comprehensive guide covers:</p> <ul> <li>Multi-topic research workflows</li> <li>Content analysis and sentiment detection</li> <li>Source comparison and coverage analysis</li> <li>Time-based analysis and trend detection</li> <li>Error handling best practices</li> <li>Integration with Jupyter notebooks</li> <li>Large dataset management strategies</li> <li>Troubleshooting common issues</li> </ul>"},{"location":"api-reference/#notes","title":"Notes","text":"<ul> <li>Prefer <code>scrapers=\"auto\"</code> unless you know which sites you need.</li> <li>Cloud/server environments are more likely to be blocked.</li> </ul> <p>Empty results: Check if your keywords are in Indonesian or try broader terms <pre><code># Too specific\ndf = nw.scrape_to_dataframe(\"very-specific-term\", \"2025-01-01\")  # Might be empty\n\n# Better approach\ndf = nw.scrape_to_dataframe(\"ekonomi,bisnis\", \"2025-01-01\")  # More likely to find articles\n</code></pre></p> <p>Timeout errors: Increase timeout for large jobs <pre><code># For large scraping jobs\ndf = nw.scrape_to_dataframe(\"politik\", \"2024-01-01\", timeout=600)  # 10 minutes\n</code></pre></p> <p>Platform issues: Some scrapers work better on different operating systems <pre><code># Let news-watch choose appropriate scrapers\ndf = nw.scrape_to_dataframe(\"berita\", \"2025-01-01\", scrapers=\"auto\")\n</code></pre></p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to news-watch will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#050-2026-01-24","title":"[0.5.0] - 2026-01-24","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>New scrapers: IDN Times, Kumparan, Merdeka, Republika, Suara, Tirto</li> <li>CSV output hardening: write to a temporary file and rename to final output</li> <li>Regression test for CSV quoting/newline handling</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>HTTP fetching now uses a global fallback chain: aiohttp \u0018\u0012 rnet \u0018\u0012 Playwright</li> <li>Scraper network tests refactored for Linux/CI stability (explicit exclusions + reduced flakiness)</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Improved cancellation/shutdown behavior to avoid lingering processes in long runs</li> <li>Multiple scraper reliability fixes and parsing hardening across the new sources</li> </ul>"},{"location":"changelog/#documentation","title":"Documentation","text":"<ul> <li>Updated docs/README to reflect the expanded scraper coverage and troubleshooting guidance</li> </ul>"},{"location":"changelog/#040-2026-01-16","title":"[0.4.0] - 2026-01-16","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>New scrapers: CNN Indonesia, Liputan6, Tribunnews</li> <li>MkDocs documentation site and GitHub Pages deploy workflow</li> <li>Tag-based release automation (GitHub Release + PyPI publish) and Makefile release targets</li> <li>Version bump script (<code>scripts/version.py</code>) that also syncs <code>CITATION.cff</code></li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Packaging migrated to <code>pyproject.toml</code> + <code>uv</code> (lockfile-based installs)</li> <li>Package moved to <code>src/</code> layout</li> <li>Minimal network scraper test keyword switched to <code>ihsg</code> for stability</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Linux CI scraping reliability via Playwright fallback when sources are blocked or return challenge pages</li> <li>RSS fallbacks for blocked search/API endpoints across multiple scrapers</li> </ul>"},{"location":"changelog/#documentation_1","title":"Documentation","text":"<ul> <li>Updated docs and README for new scrapers, new layout, docs site, and release workflow</li> </ul>"},{"location":"changelog/#030-2025-07-22","title":"[0.3.0] - 2025-07-22","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Python API with 6 core functions and structured exception hierarchy</li> <li>Data models (Article, ScrapeResult classes) for better data handling</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Fixed incorrect news source domains and removed speculative content</li> <li>Replaced impractical shell examples with proper Python code blocks</li> <li>Consolidated documentation into comprehensive guide</li> <li>Updated main README to showcase Python API alongside CLI</li> </ul>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Async queue coordination race conditions and timeout issues</li> </ul>"},{"location":"changelog/#025-2025-01-15","title":"[0.2.5] - 2025-01-15","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Antaranews scraper support</li> <li>Error handling with custom exceptions</li> <li>Timeout handling improvements</li> <li>Date parsing improvements</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Metrotvnews scraper improvements</li> <li>Concurrency optimization for better stability</li> <li>Producer-consumer architecture for better memory management</li> <li>Input validation improvements</li> </ul>"},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Okezone scraper reliability issues</li> <li>Date extraction robustness</li> <li>Linux platform stability</li> </ul>"},{"location":"changelog/#documentation_2","title":"Documentation","text":"<ul> <li>Enhanced README with guides</li> </ul>"},{"location":"changelog/#024-2024-12-15","title":"[0.2.4] - 2024-12-15","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Multi-platform support with automatic scraper selection based on OS</li> <li>Verbose logging mode for debugging and monitoring</li> <li>Excel output format in addition to CSV</li> <li>Multiple keyword filtering with comma-separated support</li> </ul>"},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>Async architecture using aiohttp</li> <li>Error recovery with exponential backoff</li> <li>Memory efficiency through streaming output</li> </ul>"},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Date filtering accuracy</li> <li>Content extraction across different sites</li> </ul>"},{"location":"changelog/#023-2024-11-20","title":"[0.2.3] - 2024-11-20","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Playwright integration for JavaScript-heavy sites</li> <li>Content quality filtering with minimum length requirements</li> <li>Source diversity supporting 14+ Indonesian news websites</li> </ul>"},{"location":"changelog/#changed_5","title":"Changed","text":"<ul> <li>CLI interface with intuitive command-line arguments</li> <li>Output formatting standardized across all scrapers</li> </ul>"},{"location":"changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Rate limiting for website rate limits</li> <li>Character encoding for Indonesian text</li> </ul>"},{"location":"changelog/#022-2024-10-15","title":"[0.2.2] - 2024-10-15","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>Detik.com scraper</li> <li>Kompas.com scraper</li> <li>Tempo.co scraper</li> <li>Date range filtering</li> </ul>"},{"location":"changelog/#changed_6","title":"Changed","text":"<ul> <li>Base scraper architecture with unified interface</li> <li>Error handling per scraper</li> </ul>"},{"location":"changelog/#021-2024-09-10","title":"[0.2.1] - 2024-09-10","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li>CNBC Indonesia scraper</li> <li>Kontan scraper</li> <li>Bisnis.com scraper</li> </ul>"},{"location":"changelog/#fixed_6","title":"Fixed","text":"<ul> <li>URL parsing for relative and absolute URLs</li> <li>Content extraction algorithms</li> </ul>"},{"location":"changelog/#020-2024-08-05","title":"[0.2.0] - 2024-08-05","text":""},{"location":"changelog/#added_8","title":"Added","text":"<ul> <li>Async scraping engine</li> <li>Multiple news sources for Indonesian websites</li> <li>CLI interface</li> <li>CSV output</li> </ul>"},{"location":"changelog/#changed_7","title":"Changed","text":"<ul> <li>Breaking change: New CLI syntax and Python API</li> <li>Performance: 10x faster with async/await</li> <li>Architecture: Modular scraper design</li> </ul>"},{"location":"changelog/#015-2024-07-01","title":"[0.1.5] - 2024-07-01","text":""},{"location":"changelog/#added_9","title":"Added","text":"<ul> <li>Scraping functionality for select Indonesian news sites</li> <li>Keyword search with article filtering</li> <li>JSON output</li> </ul>"},{"location":"changelog/#fixed_7","title":"Fixed","text":"<ul> <li>HTTP handling for network requests</li> <li>Text encoding for Indonesian characters</li> </ul>"},{"location":"changelog/#010-2024-06-01","title":"[0.1.0] - 2024-06-01","text":""},{"location":"changelog/#added_10","title":"Added","text":"<ul> <li>Initial release with news scraping prototype</li> <li>Single source support</li> <li>Article extraction functionality</li> </ul>"},{"location":"changelog/#migration-guide","title":"Migration Guide","text":""},{"location":"changelog/#from-v024-to-v025","title":"From v0.2.4 to v0.2.5","text":"<p>Python API migration:</p> <p>Old CLI-only approach: <pre><code>newswatch --keywords \"ekonomi,politik\" --start_date 2025-01-01 --output_format xlsx\n</code></pre></p> <p>New API approach: <pre><code>import newswatch as nw\ndf = nw.scrape_to_dataframe(keywords=\"ekonomi,politik\", start_date=\"2025-01-01\")\n</code></pre></p>"},{"location":"changelog/#from-v01x-to-v02x","title":"From v0.1.x to v0.2.x","text":"<p>Breaking changes in v0.2.0:</p> <ul> <li>CLI syntax changed - Use <code>newswatch</code> instead of previous commands</li> <li>Output format - New standardized article structure</li> <li>Dependencies - Requires Python 3.10+ and async libraries</li> </ul>"},{"location":"changelog/#support","title":"Support","text":"<p>For bug reports and feature requests, please visit our GitHub Issues.</p> <p>For general questions and discussion, see our documentation.</p>"},{"location":"comprehensive-guide/","title":"News-Watch Comprehensive Guide: From Basics to Advanced","text":"<p>This comprehensive guide covers everything you need to know about using news-watch for Indonesian news research, from simple searches to sophisticated analysis workflows. We'll start with basic scraping and build up to real-world data science applications.</p>"},{"location":"comprehensive-guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Getting Started</li> <li>Basic Usage</li> <li>Working with DataFrames</li> <li>Choosing Your Sources</li> <li>Building Your First Analysis</li> <li>Time-Based Analysis</li> <li>Multi-Topic Research</li> <li>Content Analysis</li> <li>Saving Your Work</li> <li>Advanced Usage Examples</li> <li>Integration Examples</li> <li>Troubleshooting Common Issues</li> </ol>"},{"location":"comprehensive-guide/#getting-started","title":"Getting Started","text":"<p>For installation, see the Getting Started guide.</p>"},{"location":"comprehensive-guide/#your-first-scrape","title":"Your First Scrape","text":"<p>Let's start with something simple - finding recent news about Indonesian economics:</p> <pre><code>import newswatch as nw\nimport pandas as pd\n\n# Search for IHSG (Indonesia stock index) news from the past week\narticles = nw.scrape(\n    keywords=\"ihsg\",\n    start_date=\"2025-01-15\"\n)\n\nprint(f\"Found {len(articles)} articles about IHSG\")\nif articles:\n    print(\"First article title:\", articles[0]['title'])\n</code></pre> <p>Each article contains these fields:</p> Field Description <code>title</code> Article headline <code>publish_date</code> When it was published <code>author</code> Writer (might be empty) <code>content</code> Full article text <code>keyword</code> Which search term matched <code>category</code> News section (Ekonomi, Politik, etc.) <code>source</code> Website name (detik, kompas, etc.) <code>link</code> Original URL"},{"location":"comprehensive-guide/#basic-usage","title":"Basic Usage","text":""},{"location":"comprehensive-guide/#simple-news-scraping","title":"Simple News Scraping","text":"<p>Start with simple keyword searches to understand the API:</p> <pre><code>import newswatch as nw\n\n# Basic scraping with generic keywords\narticles = nw.scrape(\n    keywords=\"ekonomi\", \n    start_date=\"2025-01-15\",\n    scrapers=\"kompas\"\n)\n\nprint(f\"Found {len(articles)} economic news articles\")\n\n# Each article is a dictionary with these keys:\nif articles:\n    sample_article = articles[0]\n    print(\"Article structure:\")\n    for key, value in sample_article.items():\n        print(f\"  {key}: {str(value)[:50]}...\")\n</code></pre>"},{"location":"comprehensive-guide/#file-output","title":"File Output","text":"<p>Save results for further analysis:</p> <pre><code>import newswatch as nw\n\n# Save directly to Excel file\nnw.scrape_to_file(\n    keywords=\"pendidikan\",\n    start_date=\"2025-01-10\",\n    output_path=\"education_news.xlsx\",\n    output_format=\"xlsx\",\n    scrapers=\"tempo,antaranews\"\n)\n\nprint(\"Education news saved to education_news.xlsx\")\n\n# Save to CSV\nnw.scrape_to_file(\n    keywords=\"kesehatan\",\n    start_date=\"2025-01-10\", \n    output_path=\"health_news.csv\",\n    output_format=\"csv\",\n    scrapers=\"kompas\"\n)\n\nprint(\"Health news saved to health_news.csv\")\n\n# Save to JSON for API integration\nnw.scrape_to_file(\n    keywords=\"teknologi\",\n    start_date=\"2025-01-10\", \n    output_path=\"tech_news.json\",\n    output_format=\"json\",\n    scrapers=\"detik\"\n)\n\nprint(\"Tech news saved to tech_news.json\")\n</code></pre>"},{"location":"comprehensive-guide/#working-with-dataframes","title":"Working with DataFrames","text":"<p>For data analysis, you'll usually want a pandas DataFrame:</p> <pre><code># Get economic news as a DataFrame\ndf = nw.scrape_to_dataframe(\n    keywords=\"ekonomi,inflasi,bank\",\n    start_date=\"2025-01-01\",\n    scrapers=\"detik,kompas,cnbcindonesia\"\n)\n\n# Quick overview\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Sources: {df['source'].unique()}\")\nprint(f\"Date range: {df['publish_date'].min()} to {df['publish_date'].max()}\")\n</code></pre> <p>DataFrames are ideal for analysis workflows:</p> <pre><code>import newswatch as nw\nimport pandas as pd\n\n# Get news as a pandas DataFrame\ndf = nw.scrape_to_dataframe(\n    keywords=\"teknologi\",\n    start_date=\"2025-01-15\", \n    scrapers=\"detik,kompas\"\n)\n\nprint(f\"Retrieved {len(df)} technology articles\")\nprint(f\"Columns: {list(df.columns)}\")\n\n# Basic analysis\nif not df.empty:\n    print(f\"Sources: {df['source'].value_counts().to_dict()}\")\n    print(f\"Date range: {df['publish_date'].min()} to {df['publish_date'].max()}\")\n</code></pre>"},{"location":"comprehensive-guide/#choosing-your-sources","title":"Choosing Your Sources","text":"<p>Different news sites have different strengths. Here's how to pick:</p> <pre><code># List all available scrapers\nscrapers = nw.list_scrapers()\nprint(\"Available sources:\", scrapers)\n\n# Financial news - use business-focused sites\nfinancial_df = nw.scrape_to_dataframe(\n    keywords=\"saham,investasi,obligasi\",\n    start_date=\"2025-01-01\",\n    scrapers=\"cnbcindonesia,kontan,bisnis\"  # business publications\n)\n\n# Political news - use general news sites\npolitical_df = nw.scrape_to_dataframe(\n    keywords=\"politik,pemilu,presiden\",\n    start_date=\"2025-01-01\",\n    scrapers=\"tempo,kompas,detik\"  # mainstream media\n)\n\n# Tech news - cast a wider net\ntech_df = nw.scrape_to_dataframe(\n    keywords=\"teknologi,startup,digital\",\n    start_date=\"2025-01-01\",\n    scrapers=\"auto\"  # let the system choose\n)\n</code></pre> <p>The <code>auto</code> setting picks reliable scrapers based on your platform. Use <code>all</code> only if you need maximum coverage and don't mind potential errors.</p>"},{"location":"comprehensive-guide/#building-your-first-analysis","title":"Building Your First Analysis","text":"<p>Let's analyze Indonesian economic sentiment around key events:</p> <pre><code># Collect comprehensive economic data\neconomic_keywords = \"ekonomi,inflasi,suku bunga,rupiah,ihsg\"\n\ndf = nw.scrape_to_dataframe(\n    keywords=economic_keywords,\n    start_date=\"2025-01-01\",\n    scrapers=\"cnbcindonesia,detik,kompas,bisnis\",\n    verbose=True  # see progress\n)\n\n# Basic analysis\nprint(\"Economic News Analysis\")\nprint(\"=\" * 50)\nprint(f\"Total articles: {len(df)}\")\nprint(f\"Time period: {df['publish_date'].min()} to {df['publish_date'].max()}\")\n\n# Which topics got the most coverage?\nkeyword_counts = df['keyword'].value_counts()\nprint(\"\\nMost discussed topics:\")\nfor keyword, count in keyword_counts.head().items():\n    print(f\"  {keyword}: {count} articles\")\n\n# Which sources covered economics most?\nsource_counts = df['source'].value_counts()\nprint(\"\\nMost active sources:\")\nfor source, count in source_counts.head().items():\n    print(f\"  {source}: {count} articles\")\n</code></pre>"},{"location":"comprehensive-guide/#time-based-analysis","title":"Time-Based Analysis","text":"<p>Understanding when news breaks is crucial for financial and political analysis:</p> <pre><code>import matplotlib.pyplot as plt\n\n# Convert publish_date to datetime for analysis\ndf['publish_date'] = pd.to_datetime(df['publish_date'])\ndf['date'] = df['publish_date'].dt.date\ndf['hour'] = df['publish_date'].dt.hour\n\n# Articles per day\ndaily_counts = df.groupby('date').size()\nprint(\"Daily article counts:\")\nprint(daily_counts)\n\n# Peak publishing hours\nhourly_counts = df.groupby('hour').size()\nprint(f\"\\nPeak hour: {hourly_counts.idxmax()}:00 ({hourly_counts.max()} articles)\")\n\n# Visualize the patterns\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\ndaily_counts.plot(kind='line', marker='o')\nplt.title('Articles Per Day')\nplt.xticks(rotation=45)\n\nplt.subplot(1, 2, 2)\nhourly_counts.plot(kind='bar')\nplt.title('Articles by Hour')\nplt.xlabel('Hour of Day')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"comprehensive-guide/#date-range-analysis","title":"Date Range Analysis","text":"<p>Analyze trends over time:</p> <pre><code>import newswatch as nw\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef analyze_daily_trends(keyword, days_back=14):\n    \"\"\"Analyze daily news volume trends.\"\"\"\n\n    start_date = (datetime.now() - timedelta(days=days_back)).strftime(\"%Y-%m-%d\")\n\n    df = nw.scrape_to_dataframe(\n        keywords=keyword,\n        start_date=start_date,\n        scrapers=\"auto\"\n    )\n\n    if df.empty:\n        return f\"No data found for '{keyword}'\"\n\n    # Convert to datetime and group by date\n    df['publish_date'] = pd.to_datetime(df['publish_date'])\n    df['date'] = df['publish_date'].dt.date\n\n    daily_counts = df.groupby('date').agg({\n        'title': 'count',\n        'content': lambda x: x.str.len().mean(),\n        'source': lambda x: x.nunique()\n    }).round(0)\n\n    daily_counts.columns = ['article_count', 'avg_content_length', 'source_count']\n\n    # Calculate trend statistics\n    trend_stats = {\n        'total_days': len(daily_counts),\n        'total_articles': daily_counts['article_count'].sum(),\n        'daily_average': daily_counts['article_count'].mean(),\n        'peak_day': daily_counts['article_count'].idxmax(),\n        'peak_count': daily_counts['article_count'].max(),\n        'min_day': daily_counts['article_count'].idxmin(),\n        'min_count': daily_counts['article_count'].min()\n    }\n\n    return daily_counts, trend_stats\n\n# Analyze technology news trends\ntech_daily, tech_stats = analyze_daily_trends(\"teknologi\", days_back=10)\n\nprint(\"Technology News Trends (Last 10 Days):\")\nprint(f\"  Total articles: {tech_stats['total_articles']}\")\nprint(f\"  Daily average: {tech_stats['daily_average']:.1f}\")\nprint(f\"  Peak day: {tech_stats['peak_day']} ({tech_stats['peak_count']} articles)\")\nprint(f\"  Quiet day: {tech_stats['min_day']} ({tech_stats['min_count']} articles)\")\n\nprint(\"\\nDaily breakdown:\")\nprint(tech_daily)\n</code></pre>"},{"location":"comprehensive-guide/#multi-topic-research","title":"Multi-Topic Research","text":"<p>For comprehensive research, you often need to track multiple themes:</p> <pre><code>def research_topic_cluster(topic_name, keywords, days_back=7):\n    \"\"\"Research a specific topic cluster over time.\"\"\"\n\n    from datetime import datetime, timedelta\n    start_date = (datetime.now() - timedelta(days=days_back)).strftime(\"%Y-%m-%d\")\n\n    print(f\"Researching {topic_name}...\")\n\n    df = nw.scrape_to_dataframe(\n        keywords=keywords,\n        start_date=start_date,\n        scrapers=\"auto\"\n    )\n\n    if df.empty:\n        print(f\"  No articles found for {topic_name}\")\n        return None\n\n    # Analysis\n    print(f\"  Found {len(df)} articles\")\n    print(f\"  Top sources: {', '.join(df['source'].value_counts().head(3).index)}\")\n    print(f\"  Average article length: {df['content'].str.len().mean():.0f} characters\")\n\n    return df\n\n# Research multiple themes\ntopics = {\n    'Economy': 'ekonomi,keuangan,bank,investasi',\n    'Politics': 'politik,pemilu,pemerintah,menteri',\n    'Technology': 'teknologi,digital,startup,ai',\n    'Health': 'kesehatan,covid,vaksin,rumah sakit'\n}\n\nresults = {}\nfor topic_name, keywords in topics.items():\n    results[topic_name] = research_topic_cluster(topic_name, keywords, days_back=5)\n</code></pre>"},{"location":"comprehensive-guide/#multiple-keyword-analysis","title":"Multiple Keyword Analysis","text":"<p>Work with multiple related keywords:</p> <pre><code>import newswatch as nw\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef multi_keyword_analysis(keyword_groups, days_back=7):\n    \"\"\"Analyze multiple keyword groups over a time period.\"\"\"\n\n    start_date = (datetime.now() - timedelta(days=days_back)).strftime(\"%Y-%m-%d\")\n    all_results = []\n\n    for group_name, keywords in keyword_groups.items():\n        print(f\"Processing {group_name}...\")\n\n        try:\n            df = nw.scrape_to_dataframe(\n                keywords=keywords,\n                start_date=start_date,\n                scrapers=\"kompas,detik\"  # Use reliable sources\n            )\n\n            if not df.empty:\n                df['keyword_group'] = group_name\n                all_results.append(df)\n                print(f\"  Found {len(df)} articles\")\n            else:\n                print(f\"  No articles found\")\n\n        except Exception as e:\n            print(f\"  Error: {e}\")\n\n    if all_results:\n        combined_df = pd.concat(all_results, ignore_index=True)\n\n        # Analysis by group\n        group_analysis = combined_df.groupby('keyword_group').agg({\n            'title': 'count',\n            'content': lambda x: x.str.len().mean(),\n            'source': lambda x: x.nunique()\n        }).round(0)\n\n        group_analysis.columns = ['article_count', 'avg_content_length', 'source_count']\n\n        return combined_df, group_analysis\n\n    return pd.DataFrame(), pd.DataFrame()\n\n# Define keyword groups for analysis\nkeyword_groups = {\n    'education': 'pendidikan,sekolah,mahasiswa',\n    'technology': 'teknologi,digital,internet',\n    'environment': 'lingkungan,iklim,polusi',\n    'health': 'kesehatan,rumah sakit,dokter'\n}\n\n# Run analysis\ncombined_data, group_summary = multi_keyword_analysis(keyword_groups, days_back=5)\n\nif not group_summary.empty:\n    print(\"\\nKeyword Group Analysis:\")\n    print(group_summary)\n\n    # Save combined results\n    combined_data.to_excel(\"multi_topic_analysis.xlsx\", index=False)\n    print(f\"\\nSaved {len(combined_data)} total articles to multi_topic_analysis.xlsx\")\n</code></pre>"},{"location":"comprehensive-guide/#content-analysis","title":"Content Analysis","text":"<p>Once you have articles, you can analyze the actual content:</p> <pre><code># Basic content statistics\ndf['content_length'] = df['content'].str.len()\ndf['word_count'] = df['content'].str.split().str.len()\n\nprint(\"Content Analysis:\")\nprint(f\"Average article length: {df['content_length'].mean():.0f} characters\")\nprint(f\"Average word count: {df['word_count'].mean():.0f} words\")\nprint(f\"Longest article: {df['content_length'].max()} characters\")\nprint(f\"Shortest article: {df['content_length'].min()} characters\")\n\n# Find articles with specific terms\nurgent_articles = df[df['content'].str.contains('urgent|penting|mendesak', case=False, na=False)]\nprint(f\"\\nArticles with urgent language: {len(urgent_articles)}\")\n\n# Look for sentiment indicators\npositive_words = ['naik', 'meningkat', 'positif', 'bagus', 'menguat']\nnegative_words = ['turun', 'menurun', 'negatif', 'buruk', 'melemah']\n\nfor word in positive_words:\n    count = df['content'].str.contains(word, case=False, na=False).sum()\n    print(f\"Articles mentioning '{word}': {count}\")\n</code></pre>"},{"location":"comprehensive-guide/#content-analysis-with-metrics","title":"Content Analysis with Metrics","text":"<p>Analyze article characteristics:</p> <pre><code>import newswatch as nw\nimport pandas as pd\n\ndef analyze_content_patterns(keyword, start_date=\"2025-01-15\"):\n    \"\"\"Analyze content patterns for a given keyword.\"\"\"\n\n    df = nw.scrape_to_dataframe(\n        keywords=keyword,\n        start_date=start_date,\n        scrapers=\"auto\"  # Use all available scrapers\n    )\n\n    if df.empty:\n        return f\"No articles found for '{keyword}'\"\n\n    # Calculate content metrics\n    df['content_length'] = df['content'].str.len()\n    df['title_length'] = df['title'].str.len()\n    df['has_author'] = df['author'].notna()\n\n    analysis = {\n        'total_articles': len(df),\n        'average_content_length': df['content_length'].mean(),\n        'average_title_length': df['title_length'].mean(),\n        'articles_with_author': df['has_author'].sum(),\n        'sources_used': df['source'].nunique(),\n        'source_distribution': df['source'].value_counts().to_dict(),\n        'content_length_stats': {\n            'min': df['content_length'].min(),\n            'max': df['content_length'].max(),\n            'median': df['content_length'].median()\n        }\n    }\n\n    return analysis\n\n# Analyze different topics\neconomy_analysis = analyze_content_patterns(\"ekonomi\")\ntech_analysis = analyze_content_patterns(\"teknologi\")\n\nprint(\"Economy News Analysis:\")\nprint(f\"  Articles: {economy_analysis['total_articles']}\")\nprint(f\"  Avg length: {economy_analysis['average_content_length']:.0f} chars\")\nprint(f\"  Sources: {economy_analysis['sources_used']}\")\n\nprint(\"\\nTechnology News Analysis:\")\nprint(f\"  Articles: {tech_analysis['total_articles']}\")\nprint(f\"  Avg length: {tech_analysis['average_content_length']:.0f} chars\")\nprint(f\"  Sources: {tech_analysis['sources_used']}\")\n</code></pre>"},{"location":"comprehensive-guide/#source-comparison","title":"Source Comparison","text":"<p>Compare coverage across different news sources:</p> <pre><code>import newswatch as nw\nimport pandas as pd\n\ndef compare_source_coverage(keywords, sources, start_date=\"2025-01-15\"):\n    \"\"\"Compare how different sources cover the same topics.\"\"\"\n\n    results = {}\n\n    for source in sources:\n        try:\n            df = nw.scrape_to_dataframe(\n                keywords=keywords,\n                start_date=start_date,\n                scrapers=source\n            )\n\n            results[source] = {\n                'articles_found': len(df),\n                'avg_content_length': df['content'].str.len().mean() if not df.empty else 0,\n                'unique_titles': df['title'].nunique() if not df.empty else 0,\n                'status': 'success'\n            }\n\n        except Exception as e:\n            results[source] = {\n                'status': 'error',\n                'error': str(e)\n            }\n\n    return results\n\n# Compare coverage of education topics across sources  \neducation_coverage = compare_source_coverage(\n    keywords=\"pendidikan,sekolah,universitas\",\n    sources=[\"kompas\", \"detik\", \"tempo\", \"antaranews\"]\n)\n\nprint(\"Education Coverage Comparison:\")\nfor source, data in education_coverage.items():\n    if data['status'] == 'success':\n        print(f\"  {source}: {data['articles_found']} articles, \"\n              f\"avg {data['avg_content_length']:.0f} chars\")\n    else:\n        print(f\"  {source}: Error - {data['error']}\")\n</code></pre>"},{"location":"comprehensive-guide/#saving-your-work","title":"Saving Your Work","text":"<p>Different output formats serve different purposes:</p> <pre><code># For data analysis - use CSV\nnw.scrape_to_file(\n    keywords=\"teknologi,startup\",\n    start_date=\"2025-01-01\",\n    output_path=\"tech_news_analysis.csv\",\n    output_format=\"csv\"\n)\n\n# For reports - use Excel with formatting\nnw.scrape_to_file(\n    keywords=\"ekonomi,politik\",\n    start_date=\"2025-01-01\",\n    output_path=\"daily_news_report.xlsx\",\n    output_format=\"xlsx\",\n    scrapers=\"kompas,detik,tempo\"\n)\n\n# For API integration - use JSON\nnw.scrape_to_file(\n    keywords=\"fintech,digital\",\n    start_date=\"2025-01-01\",\n    output_path=\"financial_tech_data.json\",\n    output_format=\"json\",\n    scrapers=\"cnbcindonesia,bisnis\"\n)\n\n# Process and save custom analysis\nsummary_df = df.groupby(['source', 'keyword']).agg({\n    'title': 'count',\n    'content_length': 'mean',\n    'publish_date': ['min', 'max']\n}).round(0)\n\nsummary_df.to_excel(\"news_summary_analysis.xlsx\")\nprint(\"Analysis saved to news_summary_analysis.xlsx\")\n</code></pre>"},{"location":"comprehensive-guide/#advanced-usage-examples","title":"Advanced Usage Examples","text":""},{"location":"comprehensive-guide/#comparative-news-analysis","title":"Comparative News Analysis","text":"<p>Compare how different sources cover the same topics:</p> <pre><code>import newswatch as nw\nimport pandas as pd\n\n# Get political coverage from major sources\npolitics = nw.scrape_to_dataframe(\n    \"politik,pemerintah,dpr\", \n    \"2025-01-01\",\n    scrapers=\"kompas,tempo,detik,cnbcindonesia\"\n)\n\n# Coverage matrix\ncoverage = politics.groupby(['source', 'keyword']).size().unstack(fill_value=0)\nprint(\"Coverage comparison:\")\nprint(coverage)\n</code></pre>"},{"location":"comprehensive-guide/#time-series-analysis","title":"Time Series Analysis","text":"<p>Track news volume over time:</p> <pre><code>import newswatch as nw\nimport matplotlib.pyplot as plt\n\n# Collect data for multiple time periods\nall_data = []\nfor days in range(1, 8):  # Last 7 days\n    daily_data = nw.quick_scrape(\"ekonomi\", days_back=days)\n    all_data.append(daily_data)\n\n# Combine and analyze trends\ndf = pd.concat(all_data, ignore_index=True).drop_duplicates()\ndaily_volume = df.groupby(df['publish_date'].dt.date).size()\n\n# Plot the trend\ndaily_volume.plot(kind='line', title='Economic News Volume Over Time')\nplt.show()\n</code></pre>"},{"location":"comprehensive-guide/#content-analysis_1","title":"Content Analysis","text":"<p>Analyze article characteristics:</p> <pre><code>import newswatch as nw\n\ntech_news = nw.scrape_to_dataframe(\"teknologi,ai,digital\", \"2025-01-01\")\n\n# Article length analysis\ntech_news['word_count'] = tech_news['content'].str.split().str.len()\ntech_news['title_length'] = tech_news['title'].str.len()\n\n# Summary by source\nsummary = tech_news.groupby('source').agg({\n    'word_count': ['mean', 'std'],\n    'title_length': 'mean',\n    'title': 'count'\n}).round(2)\n\nprint(\"Article characteristics by source:\")\nprint(summary)\n</code></pre>"},{"location":"comprehensive-guide/#error-handling-best-practices","title":"Error Handling Best Practices","text":"<p>Handle common scenarios gracefully:</p> <pre><code>import newswatch as nw\nimport pandas as pd\nfrom datetime import datetime\n\ndef robust_news_scraping(keywords, start_date, max_retries=2):\n    \"\"\"Example of robust news scraping with error handling.\"\"\"\n\n    # Validate inputs first\n    try:\n        datetime.strptime(start_date, \"%Y-%m-%d\")\n    except ValueError:\n        return {\"error\": f\"Invalid date format: {start_date}. Use YYYY-MM-DD\"}\n\n    if not keywords.strip():\n        return {\"error\": \"Keywords cannot be empty\"}\n\n    # Get available scrapers\n    try:\n        available_scrapers = nw.list_scrapers()\n    except Exception as e:\n        return {\"error\": f\"Could not get scraper list: {e}\"}\n\n    # Try multiple scraper combinations\n    scraper_groups = [\n        [\"kompas\"],\n        [\"detik\"],\n        [\"tempo\", \"antaranews\"],\n        [\"kompas\", \"detik\"]\n    ]\n\n    for attempt, scrapers in enumerate(scraper_groups, 1):\n        # Only use scrapers that are available\n        valid_scrapers = [s for s in scrapers if s in available_scrapers]\n        if not valid_scrapers:\n            continue\n\n        print(f\"Attempt {attempt}: Using {', '.join(valid_scrapers)}\")\n\n        try:\n            df = nw.scrape_to_dataframe(\n                keywords=keywords,\n                start_date=start_date,\n                scrapers=\",\".join(valid_scrapers),\n                timeout=120  # 2 minute timeout\n            )\n\n            if not df.empty:\n                result = {\n                    \"success\": True,\n                    \"articles_found\": len(df),\n                    \"sources_used\": df['source'].unique().tolist(),\n                    \"scrapers_tried\": valid_scrapers,\n                    \"attempt_number\": attempt,\n                    \"data\": df\n                }\n                print(f\"\u2705 Success: {len(df)} articles from {len(df['source'].unique())} sources\")\n                return result\n            else:\n                print(f\"\u26a0\ufe0f No articles found with {', '.join(valid_scrapers)}\")\n\n        except Exception as e:\n            print(f\"\u274c Error with {', '.join(valid_scrapers)}: {e}\")\n            if attempt &gt;= max_retries:\n                break\n            continue\n\n    return {\n        \"success\": False,\n        \"error\": \"All scraping attempts failed\",\n        \"scrapers_tried\": [group for group in scraper_groups if any(s in available_scrapers for s in group)]\n    }\n\n# Example usage with error handling\nresult = robust_news_scraping(\"ekonomi digital\", \"2025-01-15\")\n\nif result.get(\"success\"):\n    print(f\"Successfully collected {result['articles_found']} articles\")\n    # Process the data\n    df = result[\"data\"]\n    print(f\"Sources: {', '.join(result['sources_used'])}\")\nelse:\n    print(f\"Scraping failed: {result['error']}\")\n</code></pre>"},{"location":"comprehensive-guide/#utility-functions","title":"Utility Functions","text":"<p>Helpful utility functions for common tasks:</p> <pre><code>import newswatch as nw\nimport pandas as pd\n\ndef get_scraper_status():\n    \"\"\"Check which scrapers are working well.\"\"\"\n\n    scrapers = nw.list_scrapers()\n    test_keyword = \"ekonomi\"\n    test_date = \"2025-01-16\"\n\n    scraper_status = {}\n\n    for scraper in scrapers[:5]:  # Test first 5 scrapers\n        try:\n            articles = nw.scrape(\n                keywords=test_keyword,\n                start_date=test_date,\n                scrapers=scraper,\n                timeout=30\n            )\n\n            scraper_status[scraper] = {\n                \"status\": \"working\",\n                \"article_count\": len(articles),\n                \"test_keyword\": test_keyword\n            }\n\n        except Exception as e:\n            scraper_status[scraper] = {\n                \"status\": \"error\",\n                \"error\": str(e)[:100],\n                \"test_keyword\": test_keyword\n            }\n\n    return scraper_status\n\ndef quick_news_summary(keyword, days_back=3):\n    \"\"\"Get a quick summary of recent news.\"\"\"\n\n    try:\n        df = nw.quick_scrape(keyword, days_back=days_back)\n\n        if df.empty:\n            return f\"No recent news found for '{keyword}'\"\n\n        summary = {\n            \"keyword\": keyword,\n            \"period\": f\"Last {days_back} days\", \n            \"total_articles\": len(df),\n            \"sources\": df['source'].nunique(),\n            \"source_list\": df['source'].value_counts().to_dict(),\n            \"latest_article\": {\n                \"title\": df.iloc[0]['title'],\n                \"source\": df.iloc[0]['source'],\n                \"date\": str(df.iloc[0]['publish_date'])\n            }\n        }\n\n        return summary\n\n    except Exception as e:\n        return {\"error\": f\"Failed to get summary: {e}\"}\n\n# Check scraper status\nprint(\"Checking scraper status...\")\nstatus = get_scraper_status()\nfor scraper, info in status.items():\n    if info[\"status\"] == \"working\":\n        print(f\"\u2705 {scraper}: {info['article_count']} articles\")\n    else:\n        print(f\"\u274c {scraper}: {info['error']}\")\n\n# Quick news summary\nprint(\"\\nQuick tech news summary:\")\ntech_summary = quick_news_summary(\"teknologi\", days_back=2)\nif \"error\" not in tech_summary:\n    print(f\"Found {tech_summary['total_articles']} articles from {tech_summary['sources']} sources\")\n    print(f\"Latest: {tech_summary['latest_article']['title']}\")\nelse:\n    print(tech_summary[\"error\"])\n</code></pre>"},{"location":"comprehensive-guide/#integration-examples","title":"Integration Examples","text":""},{"location":"comprehensive-guide/#jupyter-notebook-workflow","title":"Jupyter Notebook Workflow","text":"<p>Complete workflow for data analysis:</p> <pre><code># Cell 1: Setup and imports\nimport newswatch as nw\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set pandas display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', 50)\n\nprint(\"news-watch Analysis Notebook\")\nprint(\"=\" * 40)\n\n# Cell 2: Data collection\ndef collect_news_data(topics, date_range_days=7):\n    \"\"\"Collect news data for multiple topics.\"\"\"\n\n    from datetime import datetime, timedelta\n    start_date = (datetime.now() - timedelta(days=date_range_days)).strftime(\"%Y-%m-%d\")\n\n    datasets = {}\n\n    for topic in topics:\n        print(f\"Collecting {topic} news...\")\n\n        try:\n            df = nw.scrape_to_dataframe(\n                keywords=topic,\n                start_date=start_date,\n                scrapers=\"kompas,detik,tempo\"\n            )\n\n            if not df.empty:\n                df['topic'] = topic\n                datasets[topic] = df\n                print(f\"  \u2705 {topic}: {len(df)} articles\")\n            else:\n                print(f\"  \u26a0\ufe0f {topic}: No articles found\")\n\n        except Exception as e:\n            print(f\"  \u274c {topic}: Error - {e}\")\n\n    return datasets\n\n# Collect data for analysis\ntopics = [\"teknologi\", \"pendidikan\", \"kesehatan\"]\nnews_datasets = collect_news_data(topics, date_range_days=5)\n\n# Cell 3: Analysis and visualization\nif news_datasets:\n    # Combine all datasets\n    all_data = pd.concat(news_datasets.values(), ignore_index=True)\n\n    print(f\"\\nTotal articles collected: {len(all_data)}\")\n    print(f\"Topics covered: {', '.join(all_data['topic'].unique())}\")\n    print(f\"Sources used: {', '.join(all_data['source'].unique())}\")\n\n    # Create visualizations\n    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n    # Topic distribution\n    topic_counts = all_data['topic'].value_counts()\n    axes[0, 0].bar(topic_counts.index, topic_counts.values)\n    axes[0, 0].set_title('Articles by Topic')\n    axes[0, 0].set_ylabel('Number of Articles')\n\n    # Source distribution\n    source_counts = all_data['source'].value_counts()\n    axes[0, 1].pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%')\n    axes[0, 1].set_title('Articles by Source')\n\n    # Content length distribution\n    all_data['content_length'] = all_data['content'].str.len()\n    axes[1, 0].hist(all_data['content_length'], bins=15, alpha=0.7)\n    axes[1, 0].set_title('Content Length Distribution')\n    axes[1, 0].set_xlabel('Characters')\n\n    # Articles by topic and source\n    topic_source = all_data.groupby(['topic', 'source']).size().unstack(fill_value=0)\n    topic_source.plot(kind='bar', ax=axes[1, 1], stacked=True)\n    axes[1, 1].set_title('Articles by Topic and Source')\n    axes[1, 1].set_ylabel('Number of Articles')\n    axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\n    plt.tight_layout()\n    plt.show()\n\n    # Cell 4: Export results\n    # Save summary statistics\n    summary_stats = all_data.groupby('topic').agg({\n        'title': 'count',\n        'content_length': ['mean', 'median'],\n        'source': 'nunique'\n    }).round(2)\n\n    summary_stats.columns = ['article_count', 'avg_content_length', 'median_content_length', 'source_count']\n\n    print(\"\\nSummary Statistics by Topic:\")\n    print(summary_stats)\n\n    # Export data\n    all_data.to_excel(\"news_analysis_results.xlsx\", index=False)\n    summary_stats.to_excel(\"news_summary_stats.xlsx\")\n\n    print(f\"\\n\ud83d\udcc1 Data exported:\")\n    print(f\"  - news_analysis_results.xlsx ({len(all_data)} articles)\")\n    print(f\"  - news_summary_stats.xlsx (summary statistics)\")\n\nelse:\n    print(\"No data collected. Please check your network connection and try again.\")\n</code></pre>"},{"location":"comprehensive-guide/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"comprehensive-guide/#when-you-get-no-results","title":"When You Get No Results","text":"<p>Don't panic! Empty results happen more often than you'd think, especially when you're getting specific with your search terms. Here's what usually works:</p> <pre><code>df = nw.scrape_to_dataframe(keywords=\"very_specific_term\", start_date=\"2025-01-01\")\n\nif df.empty:\n    print(\"Hmm, no articles found. Let's try a few things...\")\n\n    # Try broader terms first\n    broader_df = nw.scrape_to_dataframe(keywords=\"ekonomi\", start_date=\"2025-01-01\")\n\n    if not broader_df.empty:\n        print(f\"Found {len(broader_df)} articles with broader terms!\")\n        print(\"Try using more general keywords, then filter the results\")\n</code></pre> <p>The trick is starting broad and then narrowing down. Instead of searching for \"inflasi inti\", try \"inflasi\" or even just \"ekonomi\". You can always filter the results afterward. Also, try going back a bit further in time - sometimes news cycles are quieter than expected.</p> <p>If you're still getting nothing, double-check your internet connection and try switching to <code>scrapers=\"all\"</code> to cast a wider net.</p>"},{"location":"comprehensive-guide/#dealing-with-timeouts","title":"Dealing with Timeouts","text":"<p>Large searches can take a while, especially when you're pulling from multiple sources over long time periods. When things start timing out, you have a couple of options:</p> <pre><code># Give it more time to work\nlarge_df = nw.scrape_to_dataframe(\n    keywords=\"politik,ekonomi,sosial,budaya\",\n    start_date=\"2025-01-01\",\n    timeout=600,  # bump it up to 10 minutes\n    scrapers=\"auto\"\n)\n</code></pre> <p>But honestly? Sometimes it's better to break things down into smaller chunks rather than waiting forever for one massive scrape.</p>"},{"location":"comprehensive-guide/#managing-large-datasets","title":"Managing Large Datasets","text":"<p>If you're doing serious research that spans weeks or months, your computer might start struggling with memory. Here's a chunking approach that works really well:</p> <pre><code>from datetime import datetime, timedelta\n\ndef scrape_by_chunks(keywords, start_date, chunk_days=3):\n    \"\"\"Break large date ranges into manageable pieces.\"\"\"\n\n    all_results = []\n    current_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n    end_date = datetime.now()\n\n    while current_date &lt; end_date:\n        chunk_end = min(current_date + timedelta(days=chunk_days), end_date)\n\n        print(f\"Working on {current_date.strftime('%Y-%m-%d')} to {chunk_end.strftime('%Y-%m-%d')}\")\n\n        chunk_df = nw.scrape_to_dataframe(\n            keywords=keywords,\n            start_date=current_date.strftime(\"%Y-%m-%d\"),\n            scrapers=\"auto\"\n        )\n\n        if not chunk_df.empty:\n            # Clean up the dates to match exactly what we want\n            chunk_df['publish_date'] = pd.to_datetime(chunk_df['publish_date'])\n            chunk_df = chunk_df[\n                (chunk_df['publish_date'] &gt;= current_date) &amp; \n                (chunk_df['publish_date'] &lt; chunk_end)\n            ]\n            all_results.append(chunk_df)\n            print(f\"  Got {len(chunk_df)} articles\")\n        else:\n            print(\"  No articles in this period\")\n\n        current_date = chunk_end\n\n    if all_results:\n        final_df = pd.concat(all_results, ignore_index=True)\n        print(f\"\\nAll done! Total: {len(final_df)} articles\")\n        return final_df\n    else:\n        print(\"No articles found in the entire period\")\n        return pd.DataFrame()\n\n# This approach works great for historical research\nhistorical_df = scrape_by_chunks(\"ekonomi,politik\", \"2024-12-01\", chunk_days=2)\n</code></pre> <p>This approach is more reliable than scraping a large range in a single run, and it reduces the chance of losing partial results.</p>"},{"location":"comprehensive-guide/#performance-tips","title":"Performance Tips","text":"<ul> <li>Use specific scrapers instead of \"all\" when possible</li> <li>Start with recent dates to test before running large historical scrapes  </li> <li>Local environments work best - cloud platforms may have restrictions</li> <li>Reasonable timeouts - increase timeout for large scraping jobs</li> <li>Batch processing - process results in chunks for large datasets</li> </ul>"},{"location":"comprehensive-guide/#error-scenarios","title":"Error Scenarios","text":"<p>Common issues and solutions:</p> <p>Empty results: Check if your keywords are in Indonesian or try broader terms <pre><code># Too specific\ndf = nw.scrape_to_dataframe(\"very-specific-term\", \"2025-01-01\")  # Might be empty\n\n# Better approach\ndf = nw.scrape_to_dataframe(\"ekonomi,bisnis\", \"2025-01-01\")  # More likely to find articles\n</code></pre></p> <p>Timeout errors: Increase timeout for large jobs <pre><code># For large scraping jobs\ndf = nw.scrape_to_dataframe(\"politik\", \"2024-01-01\", timeout=600)  # 10 minutes\n</code></pre></p> <p>Platform issues: Some scrapers work better on different operating systems <pre><code># Let news-watch choose appropriate scrapers\ndf = nw.scrape_to_dataframe(\"berita\", \"2025-01-01\", scrapers=\"auto\")\n</code></pre></p> <p>These examples are intended to match the current API.</p>"},{"location":"getting-started/","title":"Getting Started with news-watch","text":"<p>This guide will get you up and running with news-watch in just a few minutes. We'll cover installation, basic usage, and walk through your first scraping session.</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#basic-installation","title":"Basic Installation","text":"<p>news-watch requires Python 3.10+ and uses Playwright for browser automation. Install both:</p> <pre><code>pip install news-watch\nplaywright install chromium\n</code></pre>"},{"location":"getting-started/#development-environment","title":"Development Environment","text":"<p>If you're planning to contribute or want the latest development version:</p> <pre><code># Clone and setup\ngit clone https://github.com/okkymabruri/news-watch.git\ncd news-watch\n\n# Install dependencies (recommended)\nuv sync --all-extras\nuv run playwright install chromium\n\n# Run commands/tests via uv\nuv run newswatch --list_scrapers\nuv run pytest\n</code></pre>"},{"location":"getting-started/#virtual-environment-recommended","title":"Virtual Environment (Recommended)","text":"<p>For conda users (recommended setup):</p> <pre><code>conda create -n newswatch-env python=3.9\nconda activate newswatch-env\npip install news-watch\nplaywright install chromium\n</code></pre> <p>For venv users:</p> <pre><code>python -m venv newswatch-env\nsource newswatch-env/bin/activate  # On Windows: newswatch-env\\Scripts\\activate\npip install news-watch\nplaywright install chromium\n</code></pre>"},{"location":"getting-started/#verify-installation","title":"Verify Installation","text":"<p>Test that everything works:</p> <pre><code># Check available scrapers\nnewswatch --list_scrapers\n\n# Should show something like:\n# Available scrapers: antaranews, bisnis, bloombergtechnoz, cnbcindonesia, cnnindonesia, detik, idntimes, kompas, kumparan, liputan6, merdeka, metrotvnews, okezone, republika, suara, tempo, tirto, tribunnews, viva, mediaindonesia, katadata, jawapos, kontan\n</code></pre>"},{"location":"getting-started/#your-first-scraping-session","title":"Your First Scraping Session","text":"<p>Let's start with a simple example - scraping recent news about Indonesian banks.</p>"},{"location":"getting-started/#command-line-interface","title":"Command Line Interface","text":"<p>The easiest way to get started is with the command line:</p> <pre><code># Basic usage: scrape bank-related news from January 1, 2025\nnewswatch --keywords \"bank\" --start_date \"2025-01-01\"\n\n# This will create an Excel file with your results\n# Look for: news-watch-bank-[timestamp].xlsx\n</code></pre> <p>Add more keywords and options:</p> <pre><code># Multiple keywords, specific sources, with verbose output\nnewswatch --keywords \"bank,kredit,pinjaman\" --start_date \"2025-01-01\" \\\n          --scrapers \"kompas,bisnis,detik\" --output_format \"csv\" --verbose\n\n# Save as JSON for API integration\nnewswatch --keywords \"teknologi,startup\" --start_date \"2025-01-01\" \\\n          --scrapers \"detik,kompas\" --output_format \"json\" --verbose\n</code></pre>"},{"location":"getting-started/#python-api","title":"Python API","text":"<p>For programmatic access and data analysis:</p> <pre><code>import newswatch as nw\n\n# Scrape articles and get a pandas DataFrame\ndf = nw.scrape_to_dataframe(\"bank\", \"2025-01-01\")\n\nprint(f\"Found {len(df)} articles\")\nprint(f\"Sources: {df['source'].unique()}\")\nprint(f\"Date range: {df['publish_date'].min()} to {df['publish_date'].max()}\")\n</code></pre>"},{"location":"getting-started/#understanding-the-results","title":"Understanding the Results","text":"<p>Each article includes these fields:</p> <ul> <li>title: Article headline</li> <li>author: Article author (when available)</li> <li>publish_date: Publication date and time</li> <li>content: Full article text</li> <li>keyword: Which search keyword matched this article</li> <li>category: Article category (news, business, sports, etc.)</li> <li>source: News website name</li> <li>link: Original article URL</li> </ul>"},{"location":"getting-started/#common-usage-patterns","title":"Common Usage Patterns","text":""},{"location":"getting-started/#financial-news-research","title":"Financial News Research","text":"<p>Monitor Indonesian financial markets:</p> <pre><code>import newswatch as nw\n\n# Banking sector analysis\nbanking_news = nw.scrape_to_dataframe(\n    \"bank,bca,mandiri,bri,bni\", \n    \"2025-01-01\"\n)\n\n# Compare coverage across financial news sources\nfinancial_sources = nw.scrape_to_dataframe(\n    \"ekonomi,inflasi,bi rate\", \n    \"2025-01-01\",\n    scrapers=\"bisnis,kontan,cnbcindonesia\"\n)\n</code></pre>"},{"location":"getting-started/#political-coverage-analysis","title":"Political Coverage Analysis","text":"<p>Track political developments:</p> <pre><code>import newswatch as nw\n\n# Recent political news\npolitics = nw.quick_scrape(\"politik,pemerintah,dpr\", days_back=1)\n\n# Election coverage comparison\nelection_news = nw.scrape_to_dataframe(\n    \"pemilu,pilkada,kpu\", \n    \"2025-01-01\",\n    scrapers=\"kompas,tempo,detik\"\n)\n</code></pre>"},{"location":"getting-started/#technology-and-startup-news","title":"Technology and Startup News","text":"<p>Monitor Indonesian tech scene:</p> <pre><code>import newswatch as nw\n\n# Startup and fintech news\ntech_news = nw.scrape_to_dataframe(\n    \"startup,fintech,gojek,tokopedia\", \n    \"2025-01-01\",\n    scrapers=\"teknologi.bisnis.com,detik\"\n)\n\n# Quick daily tech roundup\ndaily_tech = nw.quick_scrape(\"teknologi,digital,ai\", days_back=1)\n</code></pre>"},{"location":"getting-started/#working-with-the-data","title":"Working with the Data","text":"<p>Once you have your DataFrame, you can perform various analyses:</p> <pre><code>import newswatch as nw\nimport pandas as pd\n\n# Get the data\ndf = nw.scrape_to_dataframe(\"ekonomi\", \"2025-01-01\")\n\n# Basic analysis\nprint(\"Articles per source:\")\nprint(df['source'].value_counts())\n\nprint(\"\\nDaily article counts:\")\ndf['date'] = pd.to_datetime(df['publish_date']).dt.date\nprint(df['date'].value_counts().sort_index())\n\n# Content analysis\ndf['word_count'] = df['content'].str.split().str.len()\nprint(f\"\\nAverage article length: {df['word_count'].mean():.0f} words\")\n\n# Filter recent articles\nrecent = df[df['publish_date'] &gt;= '2025-01-15']\nprint(f\"\\nRecent articles (&gt;= Jan 15): {len(recent)}\")\n</code></pre>"},{"location":"getting-started/#command-line-options-reference","title":"Command Line Options Reference","text":"Option Description Example <code>-k, --keywords</code> Comma-separated search terms <code>\"bank,kredit,fintech\"</code> <code>-sd, --start_date</code> Start date (YYYY-MM-DD) <code>\"2025-01-01\"</code> <code>-s, --scrapers</code> Specific scrapers or \"auto\"/\"all\" <code>\"kompas,detik\"</code> <code>-of, --output_format</code> Output format: csv, xlsx, or json <code>\"csv\"</code> <code>-o, --output_path</code> Custom output file path <code>\"news-watch-output.csv\"</code> <code>-v, --verbose</code> Show detailed progress (flag only) <code>--list_scrapers</code> Show available scrapers (flag only)"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have the basics down:</p> <ol> <li>Explore the API Reference for detailed function documentation</li> <li>Check Troubleshooting if you encounter any issues</li> <li>Experiment with different keyword combinations to find the news you need</li> </ol>"},{"location":"getting-started/#performance-tips","title":"Performance Tips","text":"<ul> <li>Local is better: news-watch performs best on local machines rather than cloud environments</li> <li>Respect rate limits: Use reasonable delays between requests (built-in)</li> <li>Choose your scrapers: Use specific scrapers for better performance than \"all\"</li> <li>Start small: Test with recent dates before running large historical scrapes</li> </ul>"},{"location":"getting-started/#getting-help","title":"Getting Help","text":"<p>If you run into issues:</p> <ol> <li>Check the Troubleshooting guide</li> <li>Look at existing GitHub Issues</li> <li>Create a new issue with:</li> <li>error message</li> <li>your OS + Python version</li> <li>the command you ran</li> </ol>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#installation","title":"Installation","text":""},{"location":"troubleshooting/#playwright","title":"Playwright","text":"<p>If <code>playwright install chromium</code> fails: <pre><code># Install playwright browser for news-watch\nconda activate newswatch-env\nplaywright install chromium\n\n# For system dependencies\nplaywright install-deps chromium\n\n# For Docker/Linux environments\napt-get update &amp;&amp; apt-get install -y \\\n    libnss3 libatk-bridge2.0-0 libdrm2 libxcomposite1 \\\n    libxdamage1 libxrandr2 libgbm1 libxss1 libasound2\n</code></pre></p>"},{"location":"troubleshooting/#package-install","title":"Package install","text":"<p>If install/import fails: <pre><code># If uv is not available, fallback to pip\npip install news-watch\n\n# Development setup (recommended)\ngit clone https://github.com/okkymabruri/news-watch.git\ncd news-watch\nuv sync --all-extras\nuv run playwright install chromium\n</code></pre></p>"},{"location":"troubleshooting/#runtime","title":"Runtime","text":""},{"location":"troubleshooting/#no-results","title":"No results","text":"<p>Quick checks: <pre><code>newswatch --list_scrapers\nnewswatch --keywords indonesia --start_date 2025-01-15 -v\n</code></pre></p> <p>Common causes:</p> <ul> <li>keywords too specific \u2192 try <code>ekonomi,bisnis,indonesia</code></li> <li>date too old \u2192 try a recent date first</li> <li>blocked in cloud/Linux \u2192 try fewer scrapers or run locally</li> </ul>"},{"location":"troubleshooting/#timeout","title":"Timeout","text":"<p>Try: <pre><code>newswatch --keywords politik --start_date 2025-01-01 --scrapers \"kompas,detik\"\n</code></pre></p>"},{"location":"troubleshooting/#memory","title":"Memory","text":"<p>For large runs, write to a file: <pre><code>newswatch --keywords ekonomi --start_date 2024-01-01 --output_format xlsx\n</code></pre></p>"},{"location":"troubleshooting/#platform-notes","title":"Platform notes","text":""},{"location":"troubleshooting/#linux-cloud","title":"Linux / cloud","text":"<p>Some sites block server/cloud IPs more aggressively.</p> <p>Some sources (e.g., Tirto) may require browser automation. Ensure Playwright is installed:</p> <pre><code>playwright install chromium\n</code></pre> <p>Try: <pre><code>newswatch --keywords berita --start_date 2025-01-01\n</code></pre></p>"},{"location":"troubleshooting/#data-quality","title":"Data quality","text":""},{"location":"troubleshooting/#missingtruncated-content","title":"Missing/truncated content","text":"<p>Causes:</p> <ul> <li>HTML structure changed</li> <li>paywall</li> <li>blocked</li> </ul> <p>Check with verbose + single scraper: <pre><code>newswatch --keywords ekonomi --start_date 2025-01-01 -v\nnewswatch --keywords ekonomi --start_date 2025-01-01 --scrapers kompas -v\nnewswatch --keywords ekonomi --start_date 2025-01-01 --scrapers detik -v\n</code></pre></p>"},{"location":"troubleshooting/#duplicates","title":"Duplicates","text":"<p>Normal when multiple sites cover the same story. Deduplicate in post-processing.</p>"},{"location":"troubleshooting/#encoding","title":"Encoding","text":"<p>If text has broken characters, try another source: <pre><code>newswatch --keywords berita --start_date 2025-01-01 --scrapers \"kompas,tempo\" -v\n</code></pre></p>"},{"location":"troubleshooting/#cli","title":"CLI","text":""},{"location":"troubleshooting/#command-not-found","title":"Command not found","text":"<p>If <code>newswatch</code> is not found: <pre><code>conda activate newswatch-env\nwhich newswatch\nuv sync --all-extras\n</code></pre></p>"},{"location":"troubleshooting/#arguments","title":"Arguments","text":"<p>Check: <pre><code>newswatch --help\n</code></pre></p>"},{"location":"troubleshooting/#tests","title":"Tests","text":""},{"location":"troubleshooting/#running-tests","title":"Running tests","text":"<pre><code>pytest tests/\npytest -m network\npytest -m \"not network\"\n</code></pre>"},{"location":"troubleshooting/#reporting-bugs","title":"Reporting bugs","text":"<p>Include:</p> <ul> <li>OS + Python version</li> <li>command you ran</li> <li>full error output</li> <li>one example URL if relevant</li> </ul>"}]}